{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emotion-detection-CNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1aOqEWNaXdhiqh4vCCWUHFKh9yxRvdq1g",
      "authorship_tag": "ABX9TyNjvAdnvbFZuZ+6iC1dNsmf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BeytullahAksoy/RPA-SAPBillingAutomation/blob/main/emotion_detection_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qerFQ-PsPet",
        "outputId": "b105aef1-6d1d-4a58-aa9e-33fff1b7122a"
      },
      "source": [
        "%cd /content/drive/MyDrive/senior-project/CNNs\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/senior-project/CNNs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU7mWtc5szlR"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2GpWWlXvHd0"
      },
      "source": [
        "df = pd.read_csv(\"fer2013.csv\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z2M6rL_vNr9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "486c1b8d-fdd4-4aa0-9e8c-d327e864f670"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "umjQ_EV-vRyp",
        "outputId": "1d8202d8-16d1-4bca-b2c2-14f9ebf7c101"
      },
      "source": [
        "f = open(\"fer2013.csv\")\n",
        "f.__next__()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'emotion,pixels,Usage\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NchlCI7iwNev"
      },
      "source": [
        "data = []\n",
        "em = []"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P_jq0RawYFZ"
      },
      "source": [
        "for i in f:\n",
        "  lable, image, use = i.strip().split(\",\")##Seperating columns for each entry in the csv file(csv columns are seperated by comma,so we split by comma(,))\n",
        "  image = np.array(image.split(\" \"), dtype=\"uint8\")\n",
        "  image = image.reshape((48, 48, 1))\n",
        "  data.append(image)\n",
        "  em.append(lable)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CqM2bAFwkNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4effdbc-6681-4000-a497-b818ce7b46fc"
      },
      "source": [
        "len(em)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35887"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZiV288Twqlh"
      },
      "source": [
        "Data = np.array(data)\n",
        "Em = np.array(em)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyDP2tlLzLaV"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "EmL = LabelBinarizer().fit_transform(Em)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUXjqRPMw5d7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InMF0C8u3gWN"
      },
      "source": [
        "trainX, testX, trainY , testY = train_test_split(Data, EmL)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_O16xBZ3824"
      },
      "source": [
        "def Create_CNN_Model(h,w,d):\n",
        "  model1 = Sequential()\n",
        "  model1.add(Conv2D(32, (3,3), activation=\"relu\", input_shape = (h,w,d)))\n",
        "  model1.add(BatchNormalization())\n",
        "  model1.add(MaxPool2D(pool_size=(2,2)))\n",
        "  model1.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "  model1.add(Conv2D(64, (3,3), activation=\"relu\"))\n",
        "  model1.add(BatchNormalization())\n",
        "  model1.add(MaxPool2D(pool_size=(1,1)))\n",
        "  model1.add(Dropout(0.25))\n",
        "\n",
        "  model1.add(Conv2D(128, (3,3), activation=\"relu\"))\n",
        "  model1.add(BatchNormalization())\n",
        "  model1.add(MaxPool2D(pool_size=(1,1)))\n",
        "  model1.add(Dropout(0.25))\n",
        "\n",
        "  model1.add(Conv2D(256, (3,3), activation=\"relu\"))\n",
        "  model1.add(BatchNormalization())\n",
        "  model1.add(MaxPool2D(pool_size=(1,1)))\n",
        "  model1.add(Dropout(0.3))\n",
        "\n",
        "  model1.add(Conv2D(256, (3,3), activation=\"relu\"))\n",
        "  model1.add(BatchNormalization())\n",
        "  model1.add(MaxPool2D(pool_size=(1,1)))\n",
        "  model1.add(Dropout(0.5))\n",
        "\n",
        "  model1.add(Conv2D(512, (3,3), activation=\"relu\"))\n",
        "  model1.add(BatchNormalization())\n",
        "  model1.add(MaxPool2D(pool_size=(1,1)))\n",
        "  model1.add(Dropout(0.5))\n",
        "  model1.add(Conv2D(512, (3,3), activation=\"relu\"))\n",
        "  model1.add(BatchNormalization())\n",
        "  model1.add(MaxPool2D(pool_size=(1,1)))\n",
        "  model1.add(Dropout(0.5))\n",
        "\n",
        "  model1.add(Conv2D(1024, (3,3), activation=\"relu\"))\n",
        "  model1.add(BatchNormalization())\n",
        "  model1.add(MaxPool2D(pool_size=(1,1)))\n",
        "  model1.add(Dropout(0.5))\n",
        "\n",
        "  model1.add(Flatten())\n",
        "  model1.add(Dense(512, activation=\"relu\"))\n",
        "  model1.add(BatchNormalization())\n",
        "  model1.add(Dropout(0.5))\n",
        "  \n",
        "\n",
        "\n",
        "  model1.add(Dense(7, activation=\"softmax\"))\n",
        "  model1.compile(loss=\"categorical_crossentropy\", optimizer=\"adamax\", metrics=[\"accuracy\"])\n",
        "  return model1"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4m_Fz1F56Mu"
      },
      "source": [
        "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.2, horizontal_flip=True, fill_mode=\"nearest\")\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIm_IZVb8r-v"
      },
      "source": [
        "model = Create_CNN_Model(48,48,1)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6es9OoH16lPv",
        "outputId": "baf8b4b3-e398-4a7d-aacd-382b5ddc402e"
      },
      "source": [
        "print(\"Train Model....\")\n",
        "H = model.fit(aug.flow(trainX, trainY, batch_size=32), validation_data=(testX, testY), epochs=80, verbose=2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Model....\n",
            "Epoch 1/80\n",
            "842/842 - 127s - loss: 2.3300 - accuracy: 0.2055 - val_loss: 1.8666 - val_accuracy: 0.2911 - 127s/epoch - 150ms/step\n",
            "Epoch 2/80\n",
            "842/842 - 112s - loss: 2.0077 - accuracy: 0.2604 - val_loss: 1.6952 - val_accuracy: 0.3648 - 112s/epoch - 133ms/step\n",
            "Epoch 3/80\n",
            "842/842 - 111s - loss: 1.8438 - accuracy: 0.3128 - val_loss: 1.5975 - val_accuracy: 0.3947 - 111s/epoch - 132ms/step\n",
            "Epoch 4/80\n",
            "842/842 - 111s - loss: 1.7170 - accuracy: 0.3507 - val_loss: 1.6918 - val_accuracy: 0.4138 - 111s/epoch - 132ms/step\n",
            "Epoch 5/80\n",
            "842/842 - 111s - loss: 1.6191 - accuracy: 0.3877 - val_loss: 1.5025 - val_accuracy: 0.4601 - 111s/epoch - 132ms/step\n",
            "Epoch 6/80\n",
            "842/842 - 111s - loss: 1.5240 - accuracy: 0.4175 - val_loss: 1.4551 - val_accuracy: 0.4611 - 111s/epoch - 132ms/step\n",
            "Epoch 7/80\n",
            "842/842 - 112s - loss: 1.4662 - accuracy: 0.4386 - val_loss: 1.3549 - val_accuracy: 0.4853 - 112s/epoch - 133ms/step\n",
            "Epoch 8/80\n",
            "842/842 - 111s - loss: 1.4264 - accuracy: 0.4526 - val_loss: 1.2681 - val_accuracy: 0.5100 - 111s/epoch - 132ms/step\n",
            "Epoch 9/80\n",
            "842/842 - 111s - loss: 1.3674 - accuracy: 0.4784 - val_loss: 1.2439 - val_accuracy: 0.5266 - 111s/epoch - 131ms/step\n",
            "Epoch 10/80\n",
            "842/842 - 111s - loss: 1.3450 - accuracy: 0.4850 - val_loss: 1.2427 - val_accuracy: 0.5357 - 111s/epoch - 131ms/step\n",
            "Epoch 11/80\n",
            "842/842 - 111s - loss: 1.3228 - accuracy: 0.4957 - val_loss: 1.1938 - val_accuracy: 0.5440 - 111s/epoch - 132ms/step\n",
            "Epoch 12/80\n",
            "842/842 - 111s - loss: 1.2912 - accuracy: 0.5111 - val_loss: 1.2539 - val_accuracy: 0.5310 - 111s/epoch - 131ms/step\n",
            "Epoch 13/80\n",
            "842/842 - 111s - loss: 1.2810 - accuracy: 0.5130 - val_loss: 1.1253 - val_accuracy: 0.5763 - 111s/epoch - 131ms/step\n",
            "Epoch 14/80\n",
            "842/842 - 111s - loss: 1.2530 - accuracy: 0.5251 - val_loss: 1.1985 - val_accuracy: 0.5483 - 111s/epoch - 131ms/step\n",
            "Epoch 15/80\n",
            "842/842 - 110s - loss: 1.2382 - accuracy: 0.5273 - val_loss: 1.1703 - val_accuracy: 0.5554 - 110s/epoch - 131ms/step\n",
            "Epoch 16/80\n",
            "842/842 - 110s - loss: 1.2282 - accuracy: 0.5342 - val_loss: 1.1351 - val_accuracy: 0.5721 - 110s/epoch - 131ms/step\n",
            "Epoch 17/80\n",
            "842/842 - 110s - loss: 1.2033 - accuracy: 0.5448 - val_loss: 1.1099 - val_accuracy: 0.5740 - 110s/epoch - 131ms/step\n",
            "Epoch 18/80\n",
            "842/842 - 111s - loss: 1.1949 - accuracy: 0.5500 - val_loss: 1.0875 - val_accuracy: 0.5917 - 111s/epoch - 131ms/step\n",
            "Epoch 19/80\n",
            "842/842 - 110s - loss: 1.1669 - accuracy: 0.5598 - val_loss: 1.0996 - val_accuracy: 0.5833 - 110s/epoch - 131ms/step\n",
            "Epoch 20/80\n",
            "842/842 - 110s - loss: 1.1673 - accuracy: 0.5571 - val_loss: 1.0801 - val_accuracy: 0.5911 - 110s/epoch - 131ms/step\n",
            "Epoch 21/80\n",
            "842/842 - 111s - loss: 1.1681 - accuracy: 0.5606 - val_loss: 1.0602 - val_accuracy: 0.5971 - 111s/epoch - 131ms/step\n",
            "Epoch 22/80\n",
            "842/842 - 110s - loss: 1.1555 - accuracy: 0.5650 - val_loss: 1.0703 - val_accuracy: 0.5995 - 110s/epoch - 131ms/step\n",
            "Epoch 23/80\n",
            "842/842 - 111s - loss: 1.1427 - accuracy: 0.5703 - val_loss: 1.0442 - val_accuracy: 0.6102 - 111s/epoch - 132ms/step\n",
            "Epoch 24/80\n",
            "842/842 - 112s - loss: 1.1298 - accuracy: 0.5762 - val_loss: 1.0837 - val_accuracy: 0.5857 - 112s/epoch - 133ms/step\n",
            "Epoch 25/80\n",
            "842/842 - 111s - loss: 1.1295 - accuracy: 0.5725 - val_loss: 1.0202 - val_accuracy: 0.6180 - 111s/epoch - 131ms/step\n",
            "Epoch 26/80\n",
            "842/842 - 111s - loss: 1.1230 - accuracy: 0.5768 - val_loss: 1.0807 - val_accuracy: 0.6016 - 111s/epoch - 131ms/step\n",
            "Epoch 27/80\n",
            "842/842 - 111s - loss: 1.1211 - accuracy: 0.5774 - val_loss: 1.0248 - val_accuracy: 0.6137 - 111s/epoch - 131ms/step\n",
            "Epoch 28/80\n",
            "842/842 - 112s - loss: 1.1050 - accuracy: 0.5873 - val_loss: 0.9961 - val_accuracy: 0.6249 - 112s/epoch - 133ms/step\n",
            "Epoch 29/80\n",
            "842/842 - 111s - loss: 1.0857 - accuracy: 0.5903 - val_loss: 1.0837 - val_accuracy: 0.5935 - 111s/epoch - 131ms/step\n",
            "Epoch 30/80\n",
            "842/842 - 111s - loss: 1.0799 - accuracy: 0.5960 - val_loss: 1.0168 - val_accuracy: 0.6173 - 111s/epoch - 132ms/step\n",
            "Epoch 31/80\n",
            "842/842 - 111s - loss: 1.0835 - accuracy: 0.5929 - val_loss: 1.1926 - val_accuracy: 0.5677 - 111s/epoch - 132ms/step\n",
            "Epoch 32/80\n",
            "842/842 - 111s - loss: 1.0788 - accuracy: 0.5922 - val_loss: 1.0553 - val_accuracy: 0.6179 - 111s/epoch - 131ms/step\n",
            "Epoch 33/80\n",
            "842/842 - 111s - loss: 1.0641 - accuracy: 0.6004 - val_loss: 0.9821 - val_accuracy: 0.6353 - 111s/epoch - 132ms/step\n",
            "Epoch 34/80\n",
            "842/842 - 111s - loss: 1.0651 - accuracy: 0.6013 - val_loss: 1.0651 - val_accuracy: 0.6091 - 111s/epoch - 132ms/step\n",
            "Epoch 35/80\n",
            "842/842 - 111s - loss: 1.0629 - accuracy: 0.5991 - val_loss: 0.9964 - val_accuracy: 0.6330 - 111s/epoch - 132ms/step\n",
            "Epoch 36/80\n",
            "842/842 - 111s - loss: 1.0556 - accuracy: 0.6024 - val_loss: 1.0124 - val_accuracy: 0.6255 - 111s/epoch - 132ms/step\n",
            "Epoch 37/80\n",
            "842/842 - 111s - loss: 1.0600 - accuracy: 0.6052 - val_loss: 1.0767 - val_accuracy: 0.6041 - 111s/epoch - 132ms/step\n",
            "Epoch 38/80\n",
            "842/842 - 111s - loss: 1.0515 - accuracy: 0.6056 - val_loss: 1.0707 - val_accuracy: 0.6102 - 111s/epoch - 132ms/step\n",
            "Epoch 39/80\n",
            "842/842 - 111s - loss: 1.0454 - accuracy: 0.6081 - val_loss: 1.0270 - val_accuracy: 0.6199 - 111s/epoch - 132ms/step\n",
            "Epoch 40/80\n",
            "842/842 - 111s - loss: 1.0424 - accuracy: 0.6086 - val_loss: 0.9791 - val_accuracy: 0.6382 - 111s/epoch - 132ms/step\n",
            "Epoch 41/80\n",
            "842/842 - 111s - loss: 1.0420 - accuracy: 0.6058 - val_loss: 1.0051 - val_accuracy: 0.6330 - 111s/epoch - 132ms/step\n",
            "Epoch 42/80\n",
            "842/842 - 111s - loss: 1.0353 - accuracy: 0.6145 - val_loss: 1.0388 - val_accuracy: 0.6244 - 111s/epoch - 132ms/step\n",
            "Epoch 43/80\n",
            "842/842 - 111s - loss: 1.0332 - accuracy: 0.6109 - val_loss: 1.0220 - val_accuracy: 0.6214 - 111s/epoch - 132ms/step\n",
            "Epoch 44/80\n",
            "842/842 - 111s - loss: 1.0290 - accuracy: 0.6103 - val_loss: 0.9964 - val_accuracy: 0.6382 - 111s/epoch - 132ms/step\n",
            "Epoch 45/80\n",
            "842/842 - 111s - loss: 1.0233 - accuracy: 0.6139 - val_loss: 0.9930 - val_accuracy: 0.6366 - 111s/epoch - 132ms/step\n",
            "Epoch 46/80\n",
            "842/842 - 111s - loss: 1.0175 - accuracy: 0.6223 - val_loss: 0.9893 - val_accuracy: 0.6360 - 111s/epoch - 132ms/step\n",
            "Epoch 47/80\n",
            "842/842 - 111s - loss: 1.0240 - accuracy: 0.6145 - val_loss: 0.9979 - val_accuracy: 0.6335 - 111s/epoch - 132ms/step\n",
            "Epoch 48/80\n",
            "842/842 - 111s - loss: 1.0177 - accuracy: 0.6186 - val_loss: 1.0099 - val_accuracy: 0.6324 - 111s/epoch - 132ms/step\n",
            "Epoch 49/80\n",
            "842/842 - 111s - loss: 1.0072 - accuracy: 0.6210 - val_loss: 1.0593 - val_accuracy: 0.6179 - 111s/epoch - 132ms/step\n",
            "Epoch 50/80\n",
            "842/842 - 111s - loss: 1.0112 - accuracy: 0.6210 - val_loss: 0.9880 - val_accuracy: 0.6413 - 111s/epoch - 132ms/step\n",
            "Epoch 51/80\n",
            "842/842 - 111s - loss: 1.0030 - accuracy: 0.6208 - val_loss: 0.9796 - val_accuracy: 0.6417 - 111s/epoch - 131ms/step\n",
            "Epoch 52/80\n",
            "842/842 - 110s - loss: 0.9886 - accuracy: 0.6293 - val_loss: 0.9705 - val_accuracy: 0.6504 - 110s/epoch - 131ms/step\n",
            "Epoch 53/80\n",
            "842/842 - 111s - loss: 0.9870 - accuracy: 0.6298 - val_loss: 0.9561 - val_accuracy: 0.6481 - 111s/epoch - 131ms/step\n",
            "Epoch 54/80\n",
            "842/842 - 110s - loss: 0.9902 - accuracy: 0.6309 - val_loss: 0.9533 - val_accuracy: 0.6533 - 110s/epoch - 131ms/step\n",
            "Epoch 55/80\n",
            "842/842 - 110s - loss: 0.9897 - accuracy: 0.6295 - val_loss: 0.9591 - val_accuracy: 0.6566 - 110s/epoch - 131ms/step\n",
            "Epoch 56/80\n",
            "842/842 - 110s - loss: 0.9885 - accuracy: 0.6317 - val_loss: 0.9462 - val_accuracy: 0.6606 - 110s/epoch - 131ms/step\n",
            "Epoch 57/80\n",
            "842/842 - 111s - loss: 0.9791 - accuracy: 0.6321 - val_loss: 0.9862 - val_accuracy: 0.6479 - 111s/epoch - 131ms/step\n",
            "Epoch 58/80\n",
            "842/842 - 110s - loss: 0.9786 - accuracy: 0.6343 - val_loss: 0.9573 - val_accuracy: 0.6539 - 110s/epoch - 131ms/step\n",
            "Epoch 59/80\n",
            "842/842 - 110s - loss: 0.9695 - accuracy: 0.6402 - val_loss: 1.0396 - val_accuracy: 0.6301 - 110s/epoch - 131ms/step\n",
            "Epoch 60/80\n",
            "842/842 - 110s - loss: 0.9623 - accuracy: 0.6406 - val_loss: 0.9844 - val_accuracy: 0.6387 - 110s/epoch - 131ms/step\n",
            "Epoch 61/80\n",
            "842/842 - 111s - loss: 0.9676 - accuracy: 0.6386 - val_loss: 0.9741 - val_accuracy: 0.6449 - 111s/epoch - 131ms/step\n",
            "Epoch 62/80\n",
            "842/842 - 110s - loss: 0.9666 - accuracy: 0.6383 - val_loss: 0.9716 - val_accuracy: 0.6492 - 110s/epoch - 131ms/step\n",
            "Epoch 63/80\n",
            "842/842 - 110s - loss: 0.9608 - accuracy: 0.6392 - val_loss: 1.0417 - val_accuracy: 0.6295 - 110s/epoch - 131ms/step\n",
            "Epoch 64/80\n",
            "842/842 - 110s - loss: 0.9650 - accuracy: 0.6411 - val_loss: 1.0327 - val_accuracy: 0.6257 - 110s/epoch - 131ms/step\n",
            "Epoch 65/80\n",
            "842/842 - 110s - loss: 0.9579 - accuracy: 0.6398 - val_loss: 0.9855 - val_accuracy: 0.6442 - 110s/epoch - 131ms/step\n",
            "Epoch 66/80\n",
            "842/842 - 110s - loss: 0.9567 - accuracy: 0.6433 - val_loss: 0.9763 - val_accuracy: 0.6481 - 110s/epoch - 131ms/step\n",
            "Epoch 67/80\n",
            "842/842 - 111s - loss: 0.9517 - accuracy: 0.6440 - val_loss: 1.0107 - val_accuracy: 0.6398 - 111s/epoch - 131ms/step\n",
            "Epoch 68/80\n",
            "842/842 - 111s - loss: 0.9534 - accuracy: 0.6433 - val_loss: 0.9965 - val_accuracy: 0.6496 - 111s/epoch - 131ms/step\n",
            "Epoch 69/80\n",
            "842/842 - 111s - loss: 0.9521 - accuracy: 0.6450 - val_loss: 0.9663 - val_accuracy: 0.6482 - 111s/epoch - 131ms/step\n",
            "Epoch 70/80\n",
            "842/842 - 110s - loss: 0.9351 - accuracy: 0.6484 - val_loss: 0.9566 - val_accuracy: 0.6579 - 110s/epoch - 131ms/step\n",
            "Epoch 71/80\n",
            "842/842 - 111s - loss: 0.9317 - accuracy: 0.6515 - val_loss: 0.9546 - val_accuracy: 0.6560 - 111s/epoch - 131ms/step\n",
            "Epoch 72/80\n",
            "842/842 - 112s - loss: 0.9375 - accuracy: 0.6486 - val_loss: 0.9517 - val_accuracy: 0.6667 - 112s/epoch - 133ms/step\n",
            "Epoch 73/80\n",
            "842/842 - 111s - loss: 0.9309 - accuracy: 0.6537 - val_loss: 0.9431 - val_accuracy: 0.6669 - 111s/epoch - 131ms/step\n",
            "Epoch 74/80\n",
            "842/842 - 111s - loss: 0.9269 - accuracy: 0.6557 - val_loss: 0.9744 - val_accuracy: 0.6528 - 111s/epoch - 131ms/step\n",
            "Epoch 75/80\n",
            "842/842 - 110s - loss: 0.9284 - accuracy: 0.6523 - val_loss: 0.9851 - val_accuracy: 0.6554 - 110s/epoch - 131ms/step\n",
            "Epoch 76/80\n",
            "842/842 - 111s - loss: 0.9362 - accuracy: 0.6469 - val_loss: 0.9673 - val_accuracy: 0.6555 - 111s/epoch - 131ms/step\n",
            "Epoch 77/80\n",
            "842/842 - 111s - loss: 0.9321 - accuracy: 0.6489 - val_loss: 0.9688 - val_accuracy: 0.6510 - 111s/epoch - 131ms/step\n",
            "Epoch 78/80\n",
            "842/842 - 112s - loss: 0.9197 - accuracy: 0.6542 - val_loss: 0.9377 - val_accuracy: 0.6691 - 112s/epoch - 133ms/step\n",
            "Epoch 79/80\n",
            "842/842 - 111s - loss: 0.8843 - accuracy: 0.6675 - val_loss: 0.9362 - val_accuracy: 0.6681 - 111s/epoch - 131ms/step\n",
            "Epoch 80/80\n",
            "842/842 - 111s - loss: 0.8894 - accuracy: 0.6656 - val_loss: 0.9510 - val_accuracy: 0.6634 - 111s/epoch - 131ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"Face_Emotion_detection.h5\")"
      ],
      "metadata": {
        "id": "DuISlrgHyNO3"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}